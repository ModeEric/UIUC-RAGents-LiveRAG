{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5be30b47-26f5-4eac-9d4c-5cb4d1ff7f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bafc968879b4c53ad5a85f1fa7abe54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Requests:   0%|          | 0/50 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:   2%|▏         | 1/50 [00:18<15:18, 18.75s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:   4%|▍         | 2/50 [00:32<12:50, 16.06s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:   6%|▌         | 3/50 [00:50<13:16, 16.95s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:   8%|▊         | 4/50 [01:06<12:41, 16.56s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  10%|█         | 5/50 [01:23<12:23, 16.52s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  12%|█▏        | 6/50 [01:35<10:54, 14.88s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  14%|█▍        | 7/50 [01:47<10:09, 14.17s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  16%|█▌        | 8/50 [01:59<09:17, 13.28s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  18%|█▊        | 9/50 [02:14<09:29, 13.90s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  20%|██        | 10/50 [02:26<08:49, 13.25s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  22%|██▏       | 11/50 [02:47<10:14, 15.75s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  24%|██▍       | 12/50 [02:59<09:10, 14.49s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  26%|██▌       | 13/50 [03:18<09:51, 15.98s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  28%|██▊       | 14/50 [03:29<08:44, 14.57s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  30%|███       | 15/50 [03:45<08:39, 14.83s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  32%|███▏      | 16/50 [03:57<08:01, 14.15s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  34%|███▍      | 17/50 [04:11<07:39, 13.94s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  36%|███▌      | 18/50 [04:28<07:53, 14.80s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  38%|███▊      | 19/50 [04:40<07:13, 13.98s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  40%|████      | 20/50 [04:51<06:37, 13.24s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  42%|████▏     | 21/50 [05:02<06:05, 12.62s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  44%|████▍     | 22/50 [05:13<05:33, 11.90s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  46%|████▌     | 23/50 [05:23<05:12, 11.58s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  48%|████▊     | 24/50 [05:38<05:21, 12.35s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  50%|█████     | 25/50 [05:49<04:58, 11.93s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  52%|█████▏    | 26/50 [06:02<04:56, 12.35s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  54%|█████▍    | 27/50 [06:21<05:28, 14.29s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  56%|█████▌    | 28/50 [06:41<05:54, 16.11s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  58%|█████▊    | 29/50 [06:54<05:18, 15.18s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  60%|██████    | 30/50 [07:12<05:20, 16.04s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  62%|██████▏   | 31/50 [07:22<04:31, 14.31s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  64%|██████▍   | 32/50 [07:40<04:36, 15.38s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  66%|██████▌   | 33/50 [07:51<03:58, 14.02s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  68%|██████▊   | 34/50 [08:02<03:28, 13.03s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  70%|███████   | 35/50 [08:15<03:14, 12.95s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  72%|███████▏  | 36/50 [08:37<03:39, 15.66s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  74%|███████▍  | 37/50 [08:52<03:22, 15.60s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  76%|███████▌  | 38/50 [09:04<02:53, 14.42s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  78%|███████▊  | 39/50 [09:17<02:34, 14.05s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  80%|████████  | 40/50 [09:31<02:19, 13.92s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  82%|████████▏ | 41/50 [09:46<02:08, 14.27s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  84%|████████▍ | 42/50 [10:05<02:06, 15.87s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  86%|████████▌ | 43/50 [10:16<01:40, 14.39s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  88%|████████▊ | 44/50 [10:29<01:22, 13.78s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  90%|█████████ | 45/50 [10:46<01:14, 14.89s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  92%|█████████▏| 46/50 [10:59<00:57, 14.32s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  94%|█████████▍| 47/50 [11:11<00:40, 13.61s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  96%|█████████▌| 48/50 [11:25<00:27, 13.79s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests:  98%|█████████▊| 49/50 [11:38<00:13, 13.50s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Requests: 100%|██████████| 50/50 [11:47<00:00, 14.15s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import statistics\n",
    "from functools import cache\n",
    "from typing import List, Tuple, Union\n",
    "from difflib import SequenceMatcher\n",
    "from pathlib import Path\n",
    "import json\n",
    "ANSWER_SCHEMA = {\n",
    "    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "    \"title\": \"Answer file schema\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"id\":        {\"type\": \"integer\", \"description\": \"Question ID\"},\n",
    "        \"question\":  {\"type\": \"string\",  \"description\": \"The question\"},\n",
    "        \"passages\":  {\n",
    "            \"type\": \"array\",\n",
    "            \"description\": \"Passages used and related FineWeb doc IDs, ordered by decreasing importance\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"passage\": {\"type\": \"string\",  \"description\": \"Passage text\"},\n",
    "                    \"doc_IDs\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"description\": \"Passage related FineWeb doc IDs, ordered by decreasing importance\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"FineWeb doc ID, e.g., <urn:uuid:d69cbebc-133a-4ebe-9378-68235ec9f091>\"\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"passage\", \"doc_IDs\"]\n",
    "            }\n",
    "        },\n",
    "        \"final_prompt\": {\"type\": \"string\", \"description\": \"Final prompt, as submitted to Falcon LLM\"},\n",
    "        \"answer\":       {\"type\": \"string\", \"description\": \"Your answer\"}\n",
    "    },\n",
    "    \"required\": [\"id\", \"question\", \"passages\", \"final_prompt\", \"answer\"]\n",
    "}\n",
    "\n",
    "def make_passage_objects(ranked):\n",
    "    \"\"\"\n",
    "    ranked: list[tuple[str passage_id, str passage_text, float score]]\n",
    "            sorted by score (high → low)\n",
    "    \"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"passage\": txt,\n",
    "            \"doc_IDs\": [pid]\n",
    "        }\n",
    "        for pid, txt, _ in ranked\n",
    "    ]\n",
    "\n",
    "\n",
    "import boto3\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoConfig,\n",
    "    AutoModelForSeq2SeqLM, AutoModelForCausalLM,\n",
    "    GenerationConfig, AutoModel\n",
    ")\n",
    "from sentence_transformers import CrossEncoder\n",
    "from pinecone import Pinecone\n",
    "from opensearchpy import OpenSearch, AWSV4SignerAuth, RequestsHttpConnection\n",
    "import openai\n",
    "AWS_PROFILE_NAME        = \"sigir-participant\"\n",
    "AWS_REGION_NAME         = \"us-east-1\"\n",
    "PINECONE_INDEX_NAME     = \"fineweb10bt-512-0w-e5-base-v2\"\n",
    "OPENSEARCH_INDEX_NAME   = \"fineweb10bt-512-0w-e5-base-v2\"\n",
    "PINECONE_NAMESPACE      = \"default\"\n",
    "EMBEDDING_MODEL_NAME    = \"intfloat/e5-base-v2\"\n",
    "CROSS_ENCODER_NAME      = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "LOCAL_LLM_NAME          = \"tiiuae/Falcon3-10B-Instruct\"\n",
    "SSM_PINECONE_TOKEN      = \"/pinecone/ro_token\"\n",
    "SSM_OPENSEARCH_ENDPOINT = \"/opensearch/endpoint\"\n",
    "BIG_K                   = 256\n",
    "CONTEXT_DOCS            = 8\n",
    "ALPHA                   = 1.2\n",
    "CONFIDENCE_THRESHOLD    = 0.7\n",
    "OPENAI_EVAL_MODEL       = \"gpt-4o\"\n",
    "SCORE_LLM_NAME = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "\n",
    "_score_cfg = AutoConfig.from_pretrained(SCORE_LLM_NAME)\n",
    "ScoreModel = (\n",
    "    AutoModelForSeq2SeqLM\n",
    "    if getattr(_score_cfg, \"is_encoder_decoder\", False)\n",
    "    else AutoModelForCausalLM\n",
    ")\n",
    "\n",
    "score_tokenizer = AutoTokenizer.from_pretrained(SCORE_LLM_NAME)\n",
    "score_model     = ScoreModel.from_pretrained(\n",
    "    SCORE_LLM_NAME,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "score_model.eval()\n",
    "\n",
    "grade_func = {\n",
    "    \"name\": \"grade_answer\",\n",
    "    \"description\": \"Return rubric-based scores for correctness and faithfulness, starting with an explanation for the scores\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"explanation\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Explanation behind the scores you are going to give according to the rubric\"\n",
    "            },\n",
    "            \"correctness_score\": {\n",
    "                \"type\": \"number\",\n",
    "                \"description\": \"Rubric 1.1 score (-1, 0, 1, or 2)\"\n",
    "            },\n",
    "            \"faithfulness_score\": {\n",
    "                \"type\": \"number\",\n",
    "                \"description\": \"Rubric 1.2 score (-1, 0, or 1)\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"explanation\",\"correctness_score\", \"faithfulness_score\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "LOCAL_LLM_NAME = \"tiiuae/Falcon3-10B-Instruct\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "_config = AutoConfig.from_pretrained(LOCAL_LLM_NAME)\n",
    "if getattr(_config, \"is_encoder_decoder\", False):\n",
    "    GenModel = AutoModelForSeq2SeqLM\n",
    "else:\n",
    "    GenModel = AutoModelForCausalLM\n",
    "\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(LOCAL_LLM_NAME)\n",
    "gen_model     = GenModel.from_pretrained(LOCAL_LLM_NAME, device_map=\"auto\")\n",
    "gen_model.eval()\n",
    "\n",
    "def run_llm(prompts: Union[str, List[str]], max_tokens: int = 512) -> Union[str, List[str]]:\n",
    "    if isinstance(prompts, str):\n",
    "        prompts = [prompts]\n",
    "    outs = []\n",
    "    for prompt in prompts:\n",
    "        inputs = gen_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        cfg = GenerationConfig(\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=False,\n",
    "            output_scores=False,\n",
    "            return_dict_in_generate=False\n",
    "        )\n",
    "        out_ids = gen_model.generate(**inputs, generation_config=cfg)\n",
    "        text = gen_tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "        outs.append(text)\n",
    "    return outs[0] if len(outs)==1 else outs\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import List, Tuple, Sequence\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from contextlib import nullcontext\n",
    "\n",
    "\n",
    "class QueryDocScorer:\n",
    "    \"\"\"\n",
    "    Rank passages by `log P(query | passage)` using a Hugging Face model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokenizer : transformers.PreTrainedTokenizerBase\n",
    "    model     : transformers.PreTrainedModel\n",
    "    max_passage_tokens : int\n",
    "        Passage + prompt token budget before truncation (to keep GPU usage bounded).\n",
    "    amp : bool\n",
    "        If True and running on CUDA, scores in fp16/bf16 autocast.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        model,\n",
    "        max_passage_tokens: int = 512,\n",
    "        amp: bool = True,\n",
    "    ):\n",
    "        self.tok = tokenizer\n",
    "        self.mdl = model.eval()                              # disable dropout\n",
    "        self.max_passage_tokens = max_passage_tokens\n",
    "        self.is_enc_dec = bool(getattr(model.config, \"is_encoder_decoder\", False))\n",
    "        self.amp = amp and (self.mdl.device.type == \"cuda\")\n",
    "\n",
    "        # quick alias\n",
    "        self.pad = self.tok.pad_token_id\n",
    "        if self.pad is None:          # GPT-like models sometimes lack pad; use eos\n",
    "            self.pad = self.tok.eos_token_id\n",
    "\n",
    "    def _prompt(self, passage: str) -> str:\n",
    "        \"\"\"\n",
    "        Prompt the LM to *generate a query* from the document.\n",
    "        We score how likely the real user query is under that distribution.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            \"You are a query-creation assistant. \"\n",
    "            \"Given the following document, write ONE concise question that could be \"\n",
    "            \"answered *solely* from the document. Ensure the question is directly related to the document topic.\\n\\n\"\n",
    "            f\"Document:\\n{passage.strip()}\\n\\nQuestion:\"\n",
    "        )\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _enc_dec_score(\n",
    "        self, enc_inputs, query_ids: torch.Tensor\n",
    "    ) -> torch.Tensor:  # shape (batch,)\n",
    "        \"\"\"\n",
    "        Log-likelihood for encoder-decoder models (T5/BART/UL2…).\n",
    "\n",
    "        enc_inputs : output of tokenizer() for the *prompted passage*.\n",
    "        query_ids  : (batch, seq_len) – already padded.\n",
    "        \"\"\"\n",
    "        # teacher-forcing: we predict token i given <query[:i]>\n",
    "        dec_in = query_ids[:, :-1]            # everything except final token\n",
    "        tgt    = query_ids[:, 1:]             # predict next\n",
    "\n",
    "        with (torch.cuda.amp.autocast() if self.amp else nullcontext()):\n",
    "            logits = self.mdl(\n",
    "                **enc_inputs,\n",
    "                decoder_input_ids=dec_in,\n",
    "                use_cache=False,\n",
    "                return_dict=False,\n",
    "            )[0]                              # (batch, seq-1, vocab)\n",
    "\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        token_lp  = log_probs.gather(2, tgt.unsqueeze(2)).squeeze(2)\n",
    "        mask      = tgt.ne(self.pad)\n",
    "        seq_lp    = (token_lp * mask).sum(1)          # (batch,)\n",
    "        return seq_lp\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _causal_score(\n",
    "        self, merged_ids: torch.Tensor, prompt_lens: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Log-likelihood for decoder-only models (GPT/Llama…).\n",
    "\n",
    "        merged_ids  : tokenised [prompt + query]  (batch, L)\n",
    "        prompt_lens : length of prompt part for each row  (batch,)\n",
    "        \"\"\"\n",
    "        with (torch.cuda.amp.autocast() if self.amp else nullcontext()):\n",
    "            logits = self.mdl(merged_ids, use_cache=False, return_dict=False)[0]\n",
    "\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        # Shift for causal LM: predict token t given <0..t-1>\n",
    "        tgt = merged_ids[:, 1:]\n",
    "        log_probs = log_probs[:, :-1]         # align shapes\n",
    "\n",
    "        # Mask out prompt tokens – we only score the *query* portion\n",
    "        mask = torch.ones_like(tgt, dtype=torch.bool)\n",
    "        for row, L in enumerate(prompt_lens.tolist()):\n",
    "            mask[row, : max(L - 1, 0)] = False      # −1 b/c shift\n",
    "        token_lp = log_probs.gather(2, tgt.unsqueeze(2)).squeeze(2)\n",
    "        seq_lp   = (token_lp * mask).sum(1)\n",
    "        return seq_lp\n",
    "\n",
    "    # --------------------------------------------------------------------- #\n",
    "    # PUBLIC API\n",
    "    # --------------------------------------------------------------------- #\n",
    "    def score(self, passages: Sequence[str], query: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Return list[ log P(query | passage) ] (higher = more relevant).\n",
    "\n",
    "        ● Operates on a *batch* of passages for speed.\n",
    "        \"\"\"\n",
    "        if not passages:\n",
    "            return []\n",
    "\n",
    "        prompts      = [self._prompt(p) for p in passages]\n",
    "        enc_inputs   = self.tok(\n",
    "            prompts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_passage_tokens,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.mdl.device)\n",
    "\n",
    "        if self.is_enc_dec:\n",
    "            query_ids = self.tok(\n",
    "                [query] * len(passages), padding=True, return_tensors=\"pt\"\n",
    "            ).input_ids.to(self.mdl.device)\n",
    "            scores = self._enc_dec_score(enc_inputs, query_ids)\n",
    "        else:\n",
    "            merged_texts = [p + query for p in prompts]\n",
    "            merged_ids = self.tok(\n",
    "                merged_texts, padding=True, return_tensors=\"pt\"\n",
    "            ).input_ids.to(self.mdl.device)\n",
    "\n",
    "            prompt_lens = (enc_inputs.input_ids != self.pad).sum(1)\n",
    "            scores = self._causal_score(merged_ids, prompt_lens)\n",
    "\n",
    "        return scores.tolist()\n",
    "\n",
    "    def rank(\n",
    "        self,\n",
    "        query: str,\n",
    "        passages: List[Tuple[str, str]],\n",
    "        top_k: int | None = None,\n",
    "        chunk_size: int = 256,\n",
    "    ) -> List[Tuple[str, str, float]]:\n",
    "        \"\"\"\n",
    "        Rank (doc_id, text) tuples by relevance to *query*.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[(doc_id, text, score)]  sorted by descending score.\n",
    "        \"\"\"\n",
    "        if not passages:\n",
    "            return []\n",
    "\n",
    "        doc_ids, texts = zip(*passages)\n",
    "\n",
    "        scores: list[float] = []\n",
    "        for start in range(0, len(texts), chunk_size):\n",
    "            batch_texts = texts[start : start + chunk_size]\n",
    "            scores.extend(self.score(batch_texts, query))\n",
    "\n",
    "        scored = list(zip(doc_ids, texts, scores))\n",
    "        scored.sort(key=lambda x: x[2], reverse=True)\n",
    "        return scored if top_k is None else scored[:top_k]\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Helpers & recall functions (unchanged)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# helpers.py  (or wherever _ssm() lives)\n",
    "@cache\n",
    "def _ssm(key: str) -> str:\n",
    "    sess = boto3.Session(\n",
    "        profile_name=AWS_PROFILE_NAME,\n",
    "        region_name=AWS_REGION_NAME\n",
    "    )\n",
    "    return sess.client(\"ssm\").get_parameter(\n",
    "        Name=key, WithDecryption=True\n",
    "    )[\"Parameter\"][\"Value\"]\n",
    "\n",
    "@cache\n",
    "def pinecone_index():\n",
    "    return Pinecone(api_key=_ssm(\"/pinecone/ro_token\")).Index(\"fineweb10bt-512-0w-e5-base-v2\")\n",
    "\n",
    "@cache\n",
    "def opensearch_client():\n",
    "    sess = boto3.Session(\n",
    "        profile_name=AWS_PROFILE_NAME,\n",
    "        region_name=AWS_REGION_NAME,\n",
    "    )\n",
    "    creds = sess.get_credentials()\n",
    "    auth  = AWSV4SignerAuth(creds, region=AWS_REGION_NAME)\n",
    "\n",
    "    host  = _ssm(SSM_OPENSEARCH_ENDPOINT)\n",
    "    return OpenSearch(\n",
    "        hosts=[{\"host\": host, \"port\": 443}],\n",
    "        http_auth=auth,\n",
    "        use_ssl=True,\n",
    "        verify_certs=True,\n",
    "        connection_class=RequestsHttpConnection,\n",
    "    )\n",
    "\n",
    "\n",
    "# ─── New: load a *separate* embedding model ────────────────────────────────────\n",
    "EMBEDDING_MODEL_NAME = \"intfloat/e5-base-v2\"\n",
    "\n",
    "embed_tok   = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)\n",
    "embed_model = AutoModel.from_pretrained(EMBEDDING_MODEL_NAME).to(device)\n",
    "embed_model.eval()\n",
    "\n",
    "def _mean_pool(last_hidden_state, attention_mask):\n",
    "    # sentence-transformers’ mean-pooling\n",
    "    mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "    summed = (last_hidden_state * mask).sum(dim=1)\n",
    "    counts = mask.sum(dim=1).clamp(min=1e-9)\n",
    "    return summed / counts\n",
    "\n",
    "# ─── Rewrite query_dense to use the new encoder ───────────────────────────────\n",
    "def query_dense(query: str, k: int):\n",
    "    inp = embed_tok([query], padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out   = embed_model(**inp)\n",
    "        emb_t = _mean_pool(out.last_hidden_state, inp[\"attention_mask\"])\n",
    "        emb_t = torch.nn.functional.normalize(emb_t, p=2, dim=1)  # E5 uses cosine sims\n",
    "        emb   = emb_t[0].cpu().tolist()\n",
    "\n",
    "    matches = pinecone_index().query(\n",
    "        vector=emb,\n",
    "        top_k=k,\n",
    "        include_metadata=True\n",
    "    ).matches\n",
    "    return [(m[\"id\"], m[\"metadata\"][\"text\"]) for m in matches]\n",
    "\n",
    "\n",
    "def query_sparse(keywords: str, k: int):\n",
    "    body = {\"query\":{\"match\":{\"text\":keywords}},\"size\":k}\n",
    "    hits = opensearch_client().search(index=\"fineweb10bt-512-0w-e5-base-v2\", body=body)[\"hits\"][\"hits\"]\n",
    "    return [(h[\"_id\"], h[\"_source\"][\"text\"]) for h in hits]\n",
    "\n",
    "def hybrid_recall(question: str, big_k: int=64):\n",
    "    dense  = query_dense(question, big_k)\n",
    "    sparse = query_sparse(\" \".join(re.findall(r\"\\w+\", question.lower())), big_k)\n",
    "    # simple merge:\n",
    "    ids = {d for d,_ in dense} | {s for s,_ in sparse}\n",
    "    texts = {d:t for d,t in dense} | {s:t for s,t in sparse}\n",
    "    return [(i, texts[i]) for i in list(ids)[:big_k]]\n",
    "\n",
    "def filter_duplicates(passages, threshold=0.9):\n",
    "    unique, seen = [], []\n",
    "    for pid, txt in passages:\n",
    "        if any(SequenceMatcher(None, txt, s).ratio()>threshold for s in seen):\n",
    "            continue\n",
    "        seen.append(txt); unique.append((pid,txt))\n",
    "    return unique\n",
    "\n",
    "def parse_confidence(text: str) -> Tuple[str,float]:\n",
    "    if \"Confidence:\" in text:\n",
    "        body, conf = text.rsplit(\"Confidence:\",1)\n",
    "        try: return body.strip(), float(conf.strip())\n",
    "        except: return body.strip(), 0.0\n",
    "    return text.strip(), 0.0\n",
    "import re\n",
    "\n",
    "def extract_answer(model_output: str) -> str:\n",
    "    \"\"\"\n",
    "    Return the substring that follows the *last* literal 'Answer:'.\n",
    "    Strips leading/trailing whitespace and any trailing stop tokens\n",
    "    the model might append (</s>, ###, etc.).\n",
    "    \"\"\"\n",
    "    marker = \"Answer:\"\n",
    "    idx = model_output.rfind(marker)\n",
    "    if idx == -1:                      # No marker → just strip everything\n",
    "        return model_output.strip()\n",
    "\n",
    "    # Everything after the marker\n",
    "    ans = model_output[idx + len(marker):]\n",
    "\n",
    "    # Remove common stop sequences the model sometimes adds\n",
    "    ans = re.sub(r\"(</s>|###|<<END>>|</assistant>)\\s*$\", \"\", ans, flags=re.I)\n",
    "\n",
    "    return ans.strip()\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Evaluation and main loop\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Evaluation Rubric & Prompt Template\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "RUBRIC = \"\"\"\\\n",
    "### 1.1 Correctness   (integer score ∈ 2, 1, 0, -1)\n",
    "  2 — Fully correct. Directly answers the question, no factual errors, includes all key details.\n",
    "  1 — Partly correct. Factually sound but incomplete or contains irrelevant details.\n",
    "  0 — Abstained / no answer.\n",
    " -1 — Incorrect. Contains at least one factual error or contradiction.\n",
    "\n",
    "### 1.2 Faithfulness  (integer score ∈ 1, 0, -1)\n",
    "  1 — Fully grounded. Every claim is supported by the supplied context.\n",
    "  0 — Partly grounded. Some claims are supported, but at least one is not traceable.\n",
    " -1 — Ungrounded. Most or all claims are unsupported or contradicted by the context.\n",
    "\"\"\"\n",
    "\n",
    "EVAL_TMPL = f\"\"\"You are an impartial evaluator.\n",
    "\n",
    "## Task\n",
    "Using *Context* as evidence, compare the **Generated answer** with the **Reference answer**.\n",
    "Assign integer scores for *Correctness* and *Faithfulness* exactly as defined below:\n",
    "\n",
    "{RUBRIC}\n",
    "\n",
    "## Inputs\n",
    "Context:\n",
    "{{context}}\n",
    "\n",
    "Question:\n",
    "{{question}}\n",
    "\n",
    "Generated answer:\n",
    "{{pred}}\n",
    "\n",
    "Reference answer:\n",
    "{{gold}}\n",
    "\"\"\"\n",
    "def generate_until_nonempty(prompts, *, max_tokens=384, max_tries=2):\n",
    "    \"\"\"\n",
    "    prompts : str | list[str]\n",
    "    Returns the same shape as `prompts`, regenerating up to `max_tries`\n",
    "    times when the stripped answer is empty.\n",
    "    \"\"\"\n",
    "    single = isinstance(prompts, str)\n",
    "    prompts = [prompts] if single else list(prompts)\n",
    "    tries   = 0\n",
    "    results = [\"\"] * len(prompts)\n",
    "\n",
    "    todo_idx = list(range(len(prompts)))   # indices still needing output\n",
    "    while todo_idx and tries < max_tries:\n",
    "        tries += 1\n",
    "        gens   = run_llm([prompts[i] for i in todo_idx], max_tokens=max_tokens)\n",
    "        if single: gens = [gens]           # normalise shape\n",
    "\n",
    "        # post-process & filter still-empty\n",
    "        new_todo = []\n",
    "        for i, gen in zip(todo_idx, gens):\n",
    "            ans = extract_answer(gen)\n",
    "            if ans:\n",
    "                results[i] = ans\n",
    "            else:\n",
    "                new_todo.append(i)\n",
    "        todo_idx = new_todo                # continue until all filled or max_tries\n",
    "    return results[0] if single else results\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Function-Call Argument Parsing\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def _parse_function_args(raw):\n",
    "    import ast, json, re\n",
    "\n",
    "    # 1️⃣ Coerce to dict\n",
    "    if isinstance(raw, dict):\n",
    "        data = raw\n",
    "    else:\n",
    "        try:\n",
    "            data = json.loads(raw)\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                data = ast.literal_eval(raw)\n",
    "            except:\n",
    "                # add quotes around bare keys, replace single→double quotes\n",
    "                fixed = re.sub(r\"([{\\[,]\\s*)([A-Za-z_][A-Za-z0-9_]*)\\s*:\", r'\\1\"\\2\":', raw)\n",
    "                fixed = fixed.replace(\"'\", '\"')\n",
    "                data = json.loads(fixed)\n",
    "    if not isinstance(data, dict):\n",
    "        return {}\n",
    "\n",
    "    # 2️⃣ Canonicalize key names\n",
    "    mapping = {\n",
    "        \"correctness\":       \"correctness_score\",\n",
    "        \"correctnessScore\":  \"correctness_score\",\n",
    "        \"faithfulness\":      \"faithfulness_score\",\n",
    "        \"faithfulnessScore\": \"faithfulness_score\",\n",
    "    }\n",
    "    cleaned = {}\n",
    "    for k, v in data.items():\n",
    "        key = mapping.get(k.strip('\"'), k.strip('\"'))\n",
    "        cleaned[key] = v\n",
    "    return cleaned\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# GPT-4o Evaluation via OpenAI Functions\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def evaluate_answer(question: str, pred: str, gold: str, context: str):\n",
    "    prompt = EVAL_TMPL.format(context=context, question=question, pred=pred, gold=gold)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert QA assistant. Use ONLY the information in the context; do NOT hallucinate.\"},\n",
    "        {\"role\": \"user\",   \"content\": prompt}\n",
    "    ]\n",
    "    openai.api_key = \"API_KEY_HERE\"   \n",
    "    resp = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        functions=[grade_func],\n",
    "        function_call={\"name\": \"grade_answer\"},\n",
    "        temperature=0.0\n",
    "    )\n",
    "\n",
    "    call = resp.choices[0].message.function_call\n",
    "    if not call:\n",
    "        raise RuntimeError(\"Model did not invoke grade_answer\")\n",
    "\n",
    "    args = _parse_function_args(call.arguments)\n",
    "    # ensure both scores are present\n",
    "    for key in (\"correctness_score\", \"faithfulness_score\"):\n",
    "        if key not in args:\n",
    "            raise KeyError(f\"Missing {key} in function call: {args}\")\n",
    "\n",
    "    return args[\"correctness_score\"], args[\"faithfulness_score\"]\n",
    "def evaluate_and_generate(\n",
    "        in_path: str,\n",
    "        *,\n",
    "        start_idx: int = 0,          # first entry to keep  (0-based, inclusive)\n",
    "        end_idx:   int | None = None,  # last entry to keep (inclusive).  None → no upper bound\n",
    "        out_path: str = \"answers.jsonl\",\n",
    "        max_examples: int | None = None   # optional per-run cap, applied *after* slicing\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Generate answers only for entries in the closed interval\n",
    "    [start_idx, end_idx].  Pass end_idx=None for “to the end”.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    evaluate_and_generate(\n",
    "        \"questions.jsonl\",\n",
    "        start_idx=450,\n",
    "        end_idx=500,\n",
    "        max_examples=None           # or leave out entirely\n",
    "    )\n",
    "    \"\"\"\n",
    "    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ── Load input ───────────────────────────────────────────────────────────\n",
    "    entries = (\n",
    "        [json.loads(l) for l in open(in_path) if l.strip()]\n",
    "        if in_path.lower().endswith(\".jsonl\")\n",
    "        else [\n",
    "            json.loads(chunk)\n",
    "            for chunk in re.split(r\"Request \\d+:\\n\", open(in_path).read())\n",
    "            if chunk.strip()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Keep only the desired slice\n",
    "    if end_idx is None:\n",
    "        sliced = entries[start_idx:]\n",
    "    else:\n",
    "        sliced = entries[start_idx:end_idx + 1]      # +1 because end is inclusive\n",
    "\n",
    "    # Optional global cap, applied *after* slicing\n",
    "    if max_examples is not None:\n",
    "        sliced = sliced[:max_examples]\n",
    "\n",
    "    scorer  = QueryDocScorer(score_tokenizer, score_model)\n",
    "    corr, faith = [], []\n",
    "\n",
    "    with open(out_path, \"w\") as fout, tqdm(sliced, desc=\"Requests\") as bar:\n",
    "        for i, entry in enumerate(bar, start=start_idx):   # preserve original IDs\n",
    "            try:\n",
    "                # ── Inputs ───────────────────────────────────────────────\n",
    "                q = entry.get(\n",
    "                    \"question\",\n",
    "                    entry.get(\"response\", {}).get(\"result\", [{}])[0].get(\"question\"),\n",
    "                )\n",
    "                gold = entry.get(\"response\", {}).get(\"result\", [{}])[0].get(\n",
    "                    \"answer\", None\n",
    "                )\n",
    "\n",
    "                # ── Retrieval (BIG_K) ───────────────────────────────────\n",
    "                passages = hybrid_recall(q, BIG_K)\n",
    "\n",
    "                # ── Re-ranking (CONTEXT_DOCS) ───────────────────────────\n",
    "                ranked = scorer.rank(q, passages, top_k=CONTEXT_DOCS)\n",
    "\n",
    "                passage_objs = make_passage_objects(ranked)\n",
    "\n",
    "                # ── Build context & generate answer ─────────────────────\n",
    "                context = \"\\n\\n\".join(\n",
    "                    f\"Document {j+1} (ID={pid}):\\n{txt}\"\n",
    "                    for j, (pid, txt, _) in enumerate(reversed(ranked))\n",
    "                )\n",
    "                final_prompt = (\n",
    "                    \"<|system|>Context:\\n<CONTEXT>\"\n",
    "                    f\"{context}\"\n",
    "                    \"</CONTEXT>\\n\\nSystem Prompt: You are an expert Question Answering assistant. \"\n",
    "                    \"Given a user query below, comprehensively construct an answer in 300 or fewer words that gives them the information they are most likely searching for. \"\n",
    "                    \"Use the information inside the <CONTEXT> … </CONTEXT> section to assist your answer. \"\n",
    "                    f\"Question: <|user|>{q}<|assistant|>\\nAnswer:\"\n",
    "                )\n",
    "                ans = generate_until_nonempty(final_prompt)\n",
    "\n",
    "                # ── Optional evaluation ────────────────────────────────\n",
    "                if gold is not None:\n",
    "                    c, f = evaluate_answer(q, ans, gold, context)\n",
    "                    corr.append(c)\n",
    "                    faith.append(f)\n",
    "                    bar.set_postfix_str(\n",
    "                        f\"AvgCorr={statistics.mean(corr):.2f}  \"\n",
    "                        f\"AvgFaith={statistics.mean(faith):.2f}\"\n",
    "                    )\n",
    "\n",
    "                # ── Persist ─────────────────────────────────────────────\n",
    "                rec = {\n",
    "                    \"id\":         int(entry.get(\"id\", i)),\n",
    "                    \"question\":   q,\n",
    "                    \"passages\":   passage_objs,\n",
    "                    \"final_prompt\": final_prompt,\n",
    "                    \"answer\":     ans.strip(),\n",
    "                }\n",
    "                fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"ERROR:\", e)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    evaluate_and_generate(\"questions.jsonl\", max_examples=500, start_idx=0, end_idx=500)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
